# Tools

## Update the leaderboard

run this from the project root directory:

```sh
./.tools/update-leaderboard.sh guide/src/20_leaderboard/results/*.json
```

NOTE: the json file structure this script is expecting is the one generated by the `run-benchmark.sh` script. (aka hyperfine output format), here is an example:

```json
{
  "results": [
    {
      "command": "cargo run --release -- ../samples/weather_1B.csv",
      "mean": 444.88275374361999,
      "stddev": 1.5316605606659666,
      "median": 116.45674556022,
      "user": 112.10102237999999,
      "system": 4.305615700000001,
      "min": 115.10328297672,
      "max": 119.68685843572,
      "times": [
        119.21931364372, 119.68685843572, 116.17629906072, 117.57240039372, 116.50189139372,
        116.41159972672, 116.85584872672, 115.30137347672, 115.99866960172, 115.10328297672
      ],
      "exit_codes": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    }
  ]
}
```

## Run a new benchmark

run this from the project root directory:

```sh
./.tools/run-benchmark.sh <user-a>
```

where `<user-a>` is the folder where the solution of user-a is located.